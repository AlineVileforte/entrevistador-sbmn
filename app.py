import streamlit as st
import google.generativeai as genai
from datetime import datetime
import gspread
from google.oauth2.service_account import Credentials
import json

# ConfiguraÃ§Ã£o da pÃ¡gina
st.set_page_config(
    page_title="Entrevistador SBMN v6",
    page_icon="ğŸ’¬",
    layout="wide"
)

# TÃ­tulo
st.title("ğŸ¯ Entrevistador SBMN v6")
st.markdown("*Assistente especializado em modelagem SBMN para processos de negÃ³cio*")
st.markdown("---")

# Prompt do sistema (seu prompt SBMN v6)
SYSTEM_PROMPT = """VocÃª Ã© um Analista de Processos de NegÃ³cio especializado em SBMN. 

IMPORTANTE: Siga RIGOROSAMENTE este protocolo:

FASE 1 - COLETA INICIAL (3 perguntas sequenciais):
1. Pergunte: "Qual Ã© o nome do processo que vamos modelar?"
2. Depois, pergunte: "Qual Ã© o setor ou Ã¡rea de aplicaÃ§Ã£o deste processo?"
3. Por Ãºltimo, pergunte: "Liste as principais atividades e eventos que compÃµem este processo, do inÃ­cio ao fim. Separe por vÃ­rgulas."

FASE 2 - PERGUNTAS SOBRE DEPENDÃŠNCIAS:
ApÃ³s receber a lista de atividades, faÃ§a perguntas OBJETIVAS SIM/NÃƒO sobre as relaÃ§Ãµes entre pares de atividades.

Para cada par de atividades [A] e [B], pergunte:

PERGUNTA TIPO 1 (DependÃªncia):
"A atividade '[B]' precisa esperar '[A]' terminar para poder comeÃ§ar? Responda: SIM ou NÃƒO"

Se SIM, pergunte:
"A atividade '[A]' SEMPRE acontece neste processo, ou ela Ã© OPCIONAL? Responda: SEMPRE ou OPCIONAL"
- SEMPRE = B DEP A (DependÃªncia Estrita)
- OPCIONAL = B DEPC A (DependÃªncia Circunstancial)

Se NÃƒO na pergunta de dependÃªncia, pergunte:

PERGUNTA TIPO 2 (Exclusividade):
"As atividades '[A]' e '[B]' podem acontecer JUNTAS na mesma execuÃ§Ã£o do processo? Responda: SIM ou NÃƒO"
- NÃƒO = A XOR B (sÃ£o mutuamente exclusivas)
- SIM = continue para prÃ³xima pergunta

PERGUNTA TIPO 3 (UniÃ£o):
"No processo, Ã© OBRIGATÃ“RIO que pelo menos UMA das atividades ('[A]' OU '[B]') aconteÃ§a? Responda: APENAS A | APENAS B | AMBAS | NENHUMA"
- APENAS A = A UNI B (A Ã© obrigatÃ³ria)
- APENAS B = B UNI A (B Ã© obrigatÃ³ria)
- AMBAS = A UNI B (ambas obrigatÃ³rias)
- NENHUMA = sem relaÃ§Ã£o especial

FASE 3 - APRESENTAÃ‡ÃƒO:
ApÃ³s mapear todas as relaÃ§Ãµes, apresente o MODELO SBMN no formato:

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
MODELO SBMN: [Nome do Processo]
Setor: [Setor]
Data: [Data]
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ DOMÃNIO (AFOs):
A = [nome da atividade]
B = [nome da atividade]
...

ğŸ”— SITUAÃ‡Ã•ES IDENTIFICADAS:

DEP (DependÃªncias Estritas):
â€¢ [listar]

DEPC (DependÃªncias Circunstanciais):
â€¢ [listar]

XOR (NÃ£o-CoexistÃªncias):
â€¢ [listar]

UNI (UniÃµes Inclusivas):
â€¢ [listar]

â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

REGRAS CRÃTICAS:
- FaÃ§a perguntas CURTAS e OBJETIVAS
- UMA pergunta por vez
- Aguarde resposta antes da prÃ³xima pergunta
- Use formato SIM/NÃƒO sempre que possÃ­vel
- NÃƒO faÃ§a perguntas abertas ou explicativas
- NÃƒO peÃ§a esclarecimentos desnecessÃ¡rios
- FOQUE apenas nas 3 perguntas iniciais e depois nas perguntas de dependÃªncia"""

# Configurar a API do Gemini
try:
    GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]
    genai.configure(api_key=GEMINI_API_KEY)
except Exception as e:
    st.error(f"âš ï¸ Erro ao configurar a API: {str(e)}")
    st.stop()

# Inicializar o modelo com system instruction
@st.cache_resource
def get_model():
    try:
        generation_config = {
            "temperature": 0.3,  # Reduzida para maior precisÃ£o
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": 2048,  # Reduzida para respostas mais diretas
        }

        model = genai.GenerativeModel(
            model_name='gemini-2.5-flash',
            generation_config=generation_config,
            system_instruction=SYSTEM_PROMPT
        )
        return model
    except Exception as e:
        st.error(f"Erro ao criar modelo: {str(e)}")
        return None

model = get_model()

if model is None:
    st.error("NÃ£o foi possÃ­vel inicializar o modelo. Verifique as configuraÃ§Ãµes.")
    st.stop()

# Inicializar histÃ³rico de conversa
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.chat = model.start_chat(history=[])
    # Mensagem inicial
    initial_message = "OlÃ¡! Sou o Entrevistador SBMN v6. Vou conduzi-lo atravÃ©s de uma entrevista estruturada para modelar seu processo de negÃ³cio. Vamos comeÃ§ar?"
    st.session_state.messages.append({
        "role": "assistant", 
        "content": initial_message
    })

# FunÃ§Ã£o para salvar no Google Sheets
def save_to_sheets(conversation_data):
    try:
        scopes = [
            'https://www.googleapis.com/auth/spreadsheets',
            'https://www.googleapis.com/auth/drive'
        ]

        creds_dict = json.loads(st.secrets["GOOGLE_CREDENTIALS"])
        creds = Credentials.from_service_account_info(creds_dict, scopes=scopes)
        client = gspread.authorize(creds)

        sheet = client.open_by_key(st.secrets["SHEET_ID"]).sheet1

        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        full_conversation = "\n\n".join([
            f"{msg['role'].upper()}: {msg['content']}" 
            for msg in conversation_data
        ])

        sbmn_model = ""
        for msg in reversed(conversation_data):
            if "MODELO SBMN" in msg['content']:
                sbmn_model = msg['content']
                break

        row = [timestamp, full_conversation, sbmn_model]
        sheet.append_row(row)

        return True
    except Exception as e:
        st.error(f"Erro ao salvar: {str(e)}")
        return False

# Exibir histÃ³rico
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Input do usuÃ¡rio
if prompt := st.chat_input("Digite sua resposta aqui..."):
    # Adicionar mensagem do usuÃ¡rio
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Obter resposta do modelo
    with st.chat_message("assistant"):
        with st.spinner("Pensando..."):
            try:
                response = st.session_state.chat.send_message(prompt)
                response_text = response.text
                st.markdown(response_text)
            except Exception as e:
                response_text = f"Erro ao obter resposta: {str(e)}"
                st.error(response_text)

    # Adicionar resposta ao histÃ³rico
    st.session_state.messages.append({"role": "assistant", "content": response_text})

    # Verificar se a entrevista foi concluÃ­da
    if "MODELO SBMN" in response_text and "â•â•â•â•â•â•â•â•â•â•â•" in response_text:
        with st.spinner("Salvando entrevista..."):
            if save_to_sheets(st.session_state.messages):
                st.success("âœ… Entrevista salva com sucesso!")
                st.balloons()

# BotÃ£o para reiniciar
if st.button("ğŸ”„ Reiniciar Entrevista"):
    st.session_state.messages = []
    st.session_state.chat = model.start_chat(history=[])
    st.rerun()

# Sidebar
with st.sidebar:
    st.markdown("### ğŸ“‹ Sobre")
    st.markdown("Este entrevistador usa IA para modelar processos de negÃ³cio na notaÃ§Ã£o SBMN.")
    st.markdown("### ğŸ“Š Status")
    st.metric("Mensagens trocadas", len(st.session_state.messages))

    st.markdown("---")
    st.markdown("### âš™ï¸ ConfiguraÃ§Ã£o")
    if st.button("Limpar HistÃ³rico"):
        st.session_state.messages = []
        st.session_state.chat = model.start_chat(history=[])
        st.rerun()
